{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence similarity\n",
    "Dataset: STS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\olyat\\anaconda3\\envs\\torch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\olyat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\olyat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\olyat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, losses\n",
    "\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "# vocab = set(words.words())\n",
    "from nltk.stem import *\n",
    "\n",
    "from preprocessing import clean_sentence, vocab, clean_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.integrations import TensorBoardCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"tabilab/biosses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = ds['train'].train_test_split(test_size=0.2, seed=42)\n",
    "train_validate = train_test['train'].train_test_split(test_size=0.2, seed=42)\n",
    "train_set = train_validate['train']\n",
    "validate_set = train_validate['test']\n",
    "test_set = train_test['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words = set(map(lambda word: stemmer.stem(word), words.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train, unknown_words = clean_dataset(train_set, stemmed_words)\n",
    "cleaned_validation, _ = clean_dataset(validate_set, stemmed_words)\n",
    "cleaned_test, _ = clean_dataset(test_set, stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vitro', 'chromatin/nuclear', 'LATS1', 'OCT4', 'RHIMs)', '(GEFs)', 'Oct4', 'and/or', 'plasmid', 'POU5F1', 'NSCLC', 'caspase', 'I/A', 'miR-126', 'localisation', '23]', 'G-proteins', '(e.g', '95%', 'miRNA-regulated', 'deregulation', 'tumour', 'MiR-155', 'TEL-AML1-positive', 'SOX-17', 'RNAi-mediated', 'IDH1', '(RIP1', 'pRB', 'ligase', '16', 'SWI/SNF', 'ubiquitin', 'Neuro-2a', 'hWts2', 'HOPX', 'et', '(RIP', '(together', 'LATS2', 'loss-of-function', 'GTP-bound', 'PicTar', 'ESCs', 'electroporation', '3', '(BRG-associated', '(Mirus)', 'miRNAs', '24-well', 'let-7)', 'hr', 'oncogene-driven', 'Wts2', '(APC)', 'RNAhybrid', 'miR-223', 'clear-cell', 'Arp', '90%', 'SWI/SNF-like', 'SDS-PAGE', 'PPP', 'hepatocellular', 'NH2-terminal', 'ribonucleotide', 'stem/progenitor', 'Caco-2', 'prometastatic', 'upregulation', '(see', 'pCAG-GFP', '[8]', '(Fig', 'shRNA-based', 'miR-204-miRVec', '43', 'RNA', 'Aurora-A', 'miR-146a', 'chromatin-remodeling', '2004)', 'WT1', 'Toji', 'ubiquitination', 'miRVec-miR-204', 'miR-21)', 'TFs', 'p53', '7,000', 'eukaryotic', 'coverslips', 'oncogenes', '[18]', 'cycle-dependent', 'GTPase', 'caspase-8', 'ductal', 'RIP3)', 'miR-155', 'Gata2', 'germ-cell', 'knock-down', 'zebrafish', 'tumor-suppressor', 'TNF-mediated', 'CCAAT/enhancer', 'hmC', 'transfected', 'Rho-signalling', 'Thr288', 'RIP1', 'IDH1/IDH2', '(GAPs)', 'mdm2', 'miR-373', 'PLK1', 'tumorigenic', '24', 'miR-34a', 'STK33', 'upregulated', 'downregulated', 'myeloid-specific', 'miR-204', 'co-transfected', '7.4-fold', 'TargetScan', 'PC9', 'miR-133b', 'HeLa', 'cyclin', 'TET', 'EC', '<20%', 'Lats1', 'rate-limiting', 'oncogenic', 'microarray', 'signal-regulated', 'anaphase-promoting', 'miRNA', 'miR-146b', 'Affymetrix', 'miR-15a', '286', '72', 'miR-17−5p', '(NFI-A)', 'self-renewal', '1', 'electroporated', 'GEFs', 'proteasome', 'UTR', 'necroptosis', 'interest;', 'receptor-interacting', '“cyclin', 'serine-threonine', 'epithelial-mesenchymal', 'senescence-like', 'E2F1', 'miR-21', 'BRaf', 'TBK1', 'RIPK3', 'dysregulation', 'BAF', 'genome-scale', 'box”', 'miR-16−1', 'RIPK1', 'Tet2', 'downregulation', 'large-scale', 'protein-α', 'TRAF6', 'BCL-XL/MEK', 'granulopoiesis', 'ribosome', 'transcriptomics', 'Renilla-3′', 'EGFR', 'MiR-223', 'IDH1R132H', 'so-called', 'B-ALL', 'erythropoietin', 'K-Ras-driven', 'tumorigenesis', 'IL-1', 'G2/M', 'IDH2', 'Sox11', 'long-standing', 'oncogenesis]', 'pre-miRNA', 'Arp4-related', 'RNAi', 'D1', 'H-RASV12', 'c-Raf', '(C/EBPα)', 'TET2', 'K-Ras', 'Transfection', 'pRb', 'T47D', 'non-neural', 'DIANA-microT', 'actin-related', '(AML)', '500', 'RHO-related', 'OSCC', '[', '(ATCC)', 'ng', 'GATA2', 'vivo', 'SWI/SNF-regulated', 'checkpoints', 'β-actin', '(', 'GATA6', 'carcinoma]', 'HEK293T', '10]', 'BAF53', 'APC-dependent', 'Oct-4', 'apoptotic', 'TET1/2/3', 'wild-type', 'microtubule-nucleating', 'RIP3', 'Co-transfection', 'factor)', 'miR-34', 'K-ras-dependent', 'review)', 'StemBase', 'CDK5RAP2/Cnn', '(PTC)', 'MCF7', '22]', 'respectively)', 'KRas-dependent', 'up-regulation', 'CRaf', 'S2)', 'IDH', '(CD44', 'ATP-dependent', 'Up-regulation', 'MOE430A', 'SOX2', 'NSCLCs', '5', '(RB1)', 'qRT-PCR', 'ERK', 'pSuper', 'miR-372)', 'miR-24', 'druggable', 'RXXL', 'NIH3T3', 'miR-145', 'miR-143', '5′', 'LATS2-depletion', 'TAK1', '(RIP1)', 'Oct4-associated', 'scrambled-miRVec', 'Eukaryotic', 'MCF-7', 'Tumorigenesis', 'up-regulated', 'Oct-4-dependent', 'TransIT-LT1', 'Cyclin', 'Kras-driven', 'Skbr3', 'Craf', 'Lats2/Kpm', 'p19(p14)/ARF', 'Braf', 'oncogene', 'dose-dependent', 'AML'}\n",
      "293\n"
     ]
    }
   ],
   "source": [
    "print(unknown_words)\n",
    "print(len(unknown_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, SimilarityFunction\n",
    "from sentence_transformers.losses import CoSENTLoss\n",
    "\n",
    "# Load a model to train/finetune\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize the CoSENTLoss\n",
    "# This loss requires pairs of text and a float similarity score as a label\n",
    "loss = CoSENTLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_, test_set_, name=\"BIOSSES_test\"):\n",
    "    test_evaluator = EmbeddingSimilarityEvaluator(\n",
    "        sentences1=test_set_[\"sentence1\"],\n",
    "        sentences2=test_set_[\"sentence2\"],\n",
    "        scores=test_set_[\"score\"],\n",
    "        name=name,\n",
    "    )\n",
    "    return test_evaluator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s1 = cleaned_train['sentence1']\n",
    "train_s2 = cleaned_train['sentence2']\n",
    "validation_s1 = cleaned_validation['sentence1']\n",
    "validation_s2 = cleaned_validation['sentence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_emb1 = model.encode(train_s1, normalize_embeddings=True)\n",
    "# train_emb2 = model.encode(train_s2, normalize_embeddings=True)\n",
    "# validation_emb1 = model.encode(validation_s1, normalize_embeddings=True)\n",
    "# validation_emb2 = model.encode(validation_s2, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_similarities = model.similarity_pairwise(validation_emb1, validation_emb2)\n",
    "# print(validation_similarities)\n",
    "ground_truth_validation = cleaned_validation[\"score\"]\n",
    "# print(ground_truth_validation)\n",
    "# print(loss.forward(\n",
    "#     {\"sentence1\": cleaned_validation[\"sentence1\"],\n",
    "#      \"sentence2\": cleaned_validation[\"sentence2\"]}, \n",
    "#     ground_truth_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6227, 0.7996, 0.4542, 0.7373, 0.5717, 0.9421, 0.4272, 0.3671, 0.2034,\n",
      "        0.7460, 0.5736, 0.5206, 0.8329, 0.6441, 0.5521, 0.7422, 0.3377, 0.7593,\n",
      "        0.4291, 0.3967])\n",
      "[0.20000004768371582, 0.7999999523162842, -0.30000001192092896, 0.6000000238418579, 0.5, 1.0, 0.0, -0.30000001192092896, -0.10000002384185791, 0.7000000476837158, 0.0, 0.0, 0.8999999761581421, 0.7000000476837158, 0.7999999523162842, 0.5, 0.20000004768371582, 0.6000000238418579, 0.20000004768371582, -0.30000001192092896]\n"
     ]
    }
   ],
   "source": [
    "test_similarities = model.similarity_pairwise(\n",
    "    model.encode(cleaned_test['sentence1'], normalize_embeddings=True),\n",
    "      model.encode(cleaned_test['sentence2'], normalize_embeddings=True))\n",
    "print(test_similarities)\n",
    "ground_truth_test = cleaned_test[\"score\"]\n",
    "print(ground_truth_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs=100\n",
    "per_device_train_batch_size=2\n",
    "learning_rate=1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=\"models/all-MiniLM-L6-v2/\"+str(datetime.datetime.now()).split()[1].split('.')[0].replace(':', '-'),\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=100,\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=False,  # Set to True if you have a GPU that supports BF16\n",
    "    # batch_sampler=BatchSamplers.NO_DUPLICATES,  # losses that use \"in-batch negatives\" benefit from no duplicates\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.05,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=0.05,\n",
    "    # save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=0.05,\n",
    "    run_name='{num_train_epochs}_{per_device_train_batch_size}_{learning_rate}',  # Will be used in W&B if `wandb` is installed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=cleaned_validation[\"sentence1\"],\n",
    "    sentences2=cleaned_validation[\"sentence2\"],\n",
    "    scores=cleaned_validation[\"score\"],\n",
    "    name=\"BIOSSES_validate\",\n",
    ")\n",
    "test_evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=cleaned_test[\"sentence1\"],\n",
    "    sentences2=cleaned_test[\"sentence2\"],\n",
    "    scores=cleaned_test[\"score\"],\n",
    "    name=\"BIOSSES_test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8255907202262405\n",
      "0.8178702181441257\n",
      "0.8034242921820968\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(model, cleaned_test)['BIOSSES_test_pearson_cosine'])\n",
    "print(evaluate_model(model, cleaned_train)['BIOSSES_test_pearson_cosine'])\n",
    "print(evaluate_model(model, cleaned_validation)['BIOSSES_test_pearson_cosine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_board_callback = TensorBoardCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1607/3200 [08:11<09:18,  2.85it/s]"
     ]
    }
   ],
   "source": [
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    train_dataset=cleaned_train,\n",
    "    eval_dataset=cleaned_validation,\n",
    "    loss=loss,\n",
    "    args=args,\n",
    "    evaluator=dev_evaluator,\n",
    "    callbacks=[tensor_board_callback]\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BIOSSES_test_pearson_cosine': 0.8453659209668248,\n",
       " 'BIOSSES_test_spearman_cosine': 0.807881660786404}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results = test_evaluator(model)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/all-MiniLM-L6-v2/final/2-100-1e-6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8453659209668248\n",
      "0.9390772826974033\n",
      "0.8379312901742688\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(model, cleaned_test)['BIOSSES_test_pearson_cosine'])\n",
    "print(evaluate_model(model, cleaned_train)['BIOSSES_test_pearson_cosine'])\n",
    "print(evaluate_model(model, cleaned_validation)['BIOSSES_test_pearson_cosine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
