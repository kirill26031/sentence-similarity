{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence similarity\n",
    "Dataset: STS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pc\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, losses\n",
    "\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "# vocab = set(words.words())\n",
    "from nltk.stem import *\n",
    "\n",
    "from preprocessing import clean_sentence, vocab, clean_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"tabilab/biosses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = ds['train'].train_test_split(test_size=0.2, seed=42)\n",
    "train_validate = train_test['train'].train_test_split(test_size=0.2, seed=42)\n",
    "train_set = train_validate['train']\n",
    "validate_set = train_validate['test']\n",
    "test_set = train_test['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words = set(map(lambda word: stemmer.stem(word), words.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train, unknown_words = clean_dataset(train_set, stemmed_words)\n",
    "cleaned_validation, _ = clean_dataset(validate_set, stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pre-miRNA', 'senescence-like', 'tumorigenic', 'NSCLCs', 'GEFs', 'miR-126', 'H-RASV12', 'STK33', 'TAK1', 'vitro', 'Wts2', '(GEFs)', 'anaphase-promoting', 'downregulated', '(Fig', 'microtubule-nucleating', 'BAF', 'oncogenic', '(BRG-associated', 'K-Ras-driven', '(PTC)', 'LATS2', 'interest;', '72', 'tumorigenesis', 'POU5F1', 'miRVec-miR-204', 'HOPX', 'RNAhybrid', 'upregulation', 'ligase', 'et', 'vivo', '[8]', 'RHIMs)', 'miR-15a', 'Aurora-A', 'oncogenesis]', 'IL-1', '(RIP', '90%', 'EGFR', 'p53', 'hWts2', 'TFs', 'PLK1', 'caspase', 'K-ras-dependent', 'Thr288', 'self-renewal', '(e.g', 'granulopoiesis', 'SDS-PAGE', 'hmC', 'TargetScan', '(see', '10]', 'coverslips', '[', 'proteasome', 'miR-155', 'MOE430A', 'Renilla-3′', 'PPP', 'Caco-2', '<20%', 'Cyclin', '(AML)', 'IDH1', 'TRAF6', 'myeloid-specific', 'ATP-dependent', 'RNA', 'NIH3T3', 'miR-34a', 'E2F1', 'MiR-223', 'mdm2', 'PC9', '(RB1)', 'zebrafish', 'miR-204-miRVec', 'miR-24', 'dose-dependent', 'carcinoma]', 'oncogenes', 'MCF7', 'scrambled-miRVec', 'checkpoints', 'stem/progenitor', 'box”', 'SWI/SNF-regulated', 'miR-16−1', 'T47D', 'caspase-8', 'cyclin', 'oncogene', 'TEL-AML1-positive', 'necroptosis', 'factor)', 'let-7)', 'qRT-PCR', 'p19(p14)/ARF', '1', 'loss-of-function', 'KRas-dependent', '(', 'ESCs', 'erythropoietin', 'IDH1/IDH2', 'miR-372)', '(CD44', 'Tet2', 'IDH2', 'I/A', 'miR-146b', '95%', 'Affymetrix', 'Neuro-2a', 'pCAG-GFP', 'CDK5RAP2/Cnn', 'GTP-bound', 'miR-204', 'ng', 'Arp4-related', 'hepatocellular', 'Oct-4-dependent', 'miRNAs', '(Mirus)', 'ribosome', 'miR-21', 'co-transfected', 'deregulation', 'WT1', 'miR-17−5p', 'SWI/SNF-like', 'miR-21)', 'Co-transfection', 'Sox11', 'large-scale', '3', 'protein-α', 'Braf', 'rate-limiting', 'signal-regulated', 'so-called', 'NH2-terminal', 'K-Ras', 'clear-cell', 'germ-cell', 'GATA2', '7,000', 'TET', 'TET2', '286', 'RIP1', 'MCF-7', 'chromatin-remodeling', 'Rho-signalling', '16', 'Skbr3', 'long-standing', '(APC)', 'RIP3)', 'respectively)', 'StemBase', 'ribonucleotide', 'pSuper', 'plasmid', '(GAPs)', 'Eukaryotic', 'pRb', 'microarray', 'up-regulated', 'BAF53', 'B-ALL', 'tumor-suppressor', 'G2/M', 'miR-145', 'ubiquitin', 'miR-373', 'c-Raf', 'RNAi-mediated', 'IDH1R132H', 'Lats2/Kpm', 'genome-scale', '5', '24-well', 'G-proteins', 'TBK1', 'EC', 'hr', 'GTPase', 'and/or', 'AML', 'D1', 'APC-dependent', 'OCT4', 'transfected', '“cyclin', 'cycle-dependent', 'localisation', 'upregulated', 'SWI/SNF', 'wild-type', 'Kras-driven', 'actin-related', 'up-regulation', 'miR-146a', 'ERK', 'RHO-related', '500', '43', 'HeLa', 'RNAi', 'TET1/2/3', 'Tumorigenesis', 'LATS2-depletion', 'eukaryotic', '(RIP1', 'knock-down', 'Oct4-associated', 'BRaf', 'chromatin/nuclear', 'prometastatic', 'TNF-mediated', 'downregulation', 'CRaf', 'shRNA-based', 'electroporated', '[18]', 'druggable', '(C/EBPα)', 'MiR-155', 'SOX-17', 'serine-threonine', 'CCAAT/enhancer', 'TransIT-LT1', 'BCL-XL/MEK', 'tumour', 'OSCC', 'receptor-interacting', '7.4-fold', 'SOX2', 'Oct-4', 'oncogene-driven', 'LATS1', 'review)', 'miR-133b', 'ductal', 'miR-143', 'Up-regulation', 'apoptotic', 'HEK293T', 'miRNA', 'miRNA-regulated', 'Craf', 'Oct4', 'Lats1', '5′', 'PicTar', 'Transfection', '24', 'GATA6', 'ubiquitination', 'Gata2', '2004)', 'RIPK3', 'transcriptomics', '(ATCC)', 'RIP3', 'S2)', 'Arp', 'dysregulation', 'pRB', 'miR-34', 'electroporation', 'IDH', 'NSCLC', 'DIANA-microT', 'epithelial-mesenchymal', 'miR-223', 'RXXL', '(RIP1)', 'RIPK1', 'Toji', '23]', '22]', '(NFI-A)', 'β-actin', 'UTR', '(together', 'non-neural'}\n",
      "293\n"
     ]
    }
   ],
   "source": [
    "print(unknown_words)\n",
    "print(len(unknown_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(cleaned_validation['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name xlm-roberta-base. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, SimilarityFunction\n",
    "from sentence_transformers.losses import CoSENTLoss\n",
    "\n",
    "# Load a model to train/finetune\n",
    "model = SentenceTransformer(\"xlm-roberta-base\", device=\"cuda\")\n",
    "\n",
    "# Initialize the CoSENTLoss\n",
    "# This loss requires pairs of text and a float similarity score as a label\n",
    "loss = CoSENTLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s1 = cleaned_train['sentence1']\n",
    "train_s2 = cleaned_train['sentence2']\n",
    "validation_s1 = cleaned_validation['sentence1']\n",
    "validation_s2 = cleaned_validation['sentence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emb1 = model.encode(train_s1)\n",
    "train_emb2 = model.encode(train_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_emb1 = model.encode(validation_s1)\n",
    "validation_emb2 = model.encode(validation_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_similarities = model.similarity_pairwise(validation_emb1, validation_emb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9988, 0.9987, 0.9981, 0.9977, 0.9968, 0.9985, 0.9984, 0.9983, 0.9969,\n",
       "        0.9959, 0.9972, 0.9993, 0.9974, 0.9983, 0.9976, 0.9969])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=\"models/xlm-roberta-base\",\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=False,  # Set to True if you have a GPU that supports BF16\n",
    "    # batch_sampler=BatchSamplers.NO_DUPLICATES,  # losses that use \"in-batch negatives\" benefit from no duplicates\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=1,\n",
    "    run_name=\"xlm-roberta-base\",  # Will be used in W&B if `wandb` is installed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=cleaned_validation[\"sentence1\"],\n",
    "    sentences2=cleaned_validation[\"sentence2\"],\n",
    "    scores=cleaned_validation[\"score\"],\n",
    "    name=\"sts_dev\",\n",
    ")\n",
    "results = dev_evaluator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sts_dev_pearson_cosine': 0.1882809392202916,\n",
       " 'sts_dev_spearman_cosine': 0.2639079584508967}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Roaming\\Python\\Python312\\site-packages\\accelerate\\accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "wandb: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    }
   ],
   "source": [
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    train_dataset=cleaned_train,\n",
    "    loss=loss,\n",
    "    args=args,\n",
    "    evaluator=dev_evaluator\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
