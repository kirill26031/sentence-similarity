{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence similarity\n",
    "Dataset: STS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pc\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, losses\n",
    "\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "# vocab = set(words.words())\n",
    "from nltk.stem import *\n",
    "\n",
    "from preprocessing import clean_sentence, vocab, clean_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.integrations import TensorBoardCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"tabilab/biosses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = ds['train'].train_test_split(test_size=0.2, seed=42)\n",
    "train_validate = train_test['train'].train_test_split(test_size=0.2, seed=42)\n",
    "train_set = train_validate['train']\n",
    "validate_set = train_validate['test']\n",
    "test_set = train_test['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words = set(map(lambda word: stemmer.stem(word), words.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train, unknown_words = clean_dataset(train_set, stemmed_words)\n",
    "cleaned_validation, _ = clean_dataset(validate_set, stemmed_words)\n",
    "cleaned_test, _ = clean_dataset(test_set, stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Oct4', 'p19(p14)/ARF', 'cycle-dependent', 'qRT-PCR', 'RIP1', 'upregulated', '(BRG-associated', 'miR-126', 'oncogene-driven', 'Eukaryotic', '(CD44', 'H-RASV12', 'Up-regulation', 'miR-21)', 'epithelial-mesenchymal', 'hmC', 'AML', 'miR-143', '(', 'Cyclin', 'Affymetrix', '(together', 'miR-133b', 'E2F1', 'erythropoietin', 'miR-146b', 'miR-204', 'prometastatic', 'miR-145', 'microtubule-nucleating', '[18]', 'miRNA-regulated', 'et', 'SWI/SNF-regulated', '“cyclin', 'EGFR', 'EC', 'TBK1', 'ribonucleotide', 'StemBase', 'pCAG-GFP', 'Renilla-3′', 'proteasome', 'non-neural', 'dose-dependent', 'Skbr3', 'germ-cell', 'RIPK3', 'respectively)', 'GTPase', 'B-ALL', 'TargetScan', 'miR-155', 'SDS-PAGE', 'granulopoiesis', 'factor)', 'actin-related', 'HEK293T', 'large-scale', '(PTC)', 'Neuro-2a', 'LATS2-depletion', 'APC-dependent', 'ligase', 'ubiquitin', 'T47D', 'ductal', 'miR-204-miRVec', '(RIP', 'up-regulated', 'ESCs', 'RIP3', '72', 'caspase', 'MOE430A', 'TRAF6', 'I/A', 'dysregulation', 'tumorigenic', 'BAF53', '22]', '7,000', 'NSCLC', 'miRVec-miR-204', 'senescence-like', 'oncogenic', 'let-7)', 'chromatin/nuclear', 'Lats1', 'PC9', 'self-renewal', 'Caco-2', 'review)', 'RNA', 'RNAi-mediated', 'Transfection', 'G2/M', '10]', 'CRaf', 'Kras-driven', 'miR-146a', 'interest;', 'CCAAT/enhancer', 'Oct4-associated', 'ubiquitination', '23]', 'Co-transfection', 'pre-miRNA', 'up-regulation', 'RHO-related', 'ERK', 'MCF7', 'druggable', 'Sox11', 'tumour', 'Aurora-A', 'NIH3T3', 'OCT4', 'PicTar', 'miR-372)', '(RIP1)', 'p53', '16', 'BAF', 'so-called', 'transcriptomics', 'LATS1', '7.4-fold', '90%', '(NFI-A)', 'BCL-XL/MEK', 'miRNAs', 'eukaryotic', '5′', 'Arp4-related', 'receptor-interacting', '(C/EBPα)', 'PLK1', 'downregulation', 'Toji', '(Fig', 'miR-34a', 'SWI/SNF-like', 'oncogene', 'pRb', 'stem/progenitor', 'upregulation', 'MiR-155', 'protein-α', 'c-Raf', 'plasmid', 'IDH1', 'oncogenes', 'TEL-AML1-positive', 'IL-1', '43', 'serine-threonine', '(RB1)', '1', 'electroporation', '(e.g', 'vivo', 'miR-373', 'TAK1', 'miR-24', '(RIP1', '3', 'Craf', 'G-proteins', 'KRas-dependent', 'vitro', 'coverslips', 'β-actin', 'Lats2/Kpm', 'genome-scale', 'LATS2', 'loss-of-function', 'anaphase-promoting', 'downregulated', 'MiR-223', 'shRNA-based', '95%', '286', 'POU5F1', 'hepatocellular', 'IDH1/IDH2', 'S2)', 'HeLa', 'GTP-bound', 'STK33', 'K-Ras-driven', 'Rho-signalling', 'RNAhybrid', 'GATA6', 'hr', 'carcinoma]', '5', 'signal-regulated', 'miR-223', '500', '(AML)', 'miRNA', '2004)', 'mdm2', 'RXXL', 'knock-down', 'RIP3)', 'IDH2', 'hWts2', 'transfected', 'HOPX', '(ATCC)', 'Braf', 'Oct-4', '[', '24-well', 'caspase-8', 'ng', '24', 'clear-cell', 'ATP-dependent', 'miR-15a', 'NH2-terminal', 'myeloid-specific', 'RNAi', 'D1', 'localisation', 'OSCC', 'IDH1R132H', 'SWI/SNF', 'pSuper', 'TFs', 'TransIT-LT1', 'Tet2', 'zebrafish', '[8]', '(Mirus)', 'DIANA-microT', 'box”', 'K-ras-dependent', '(GAPs)', 'tumor-suppressor', 'UTR', '(APC)', 'necroptosis', '<20%', '(see', '(GEFs)', 'ribosome', 'miR-16−1', 'Gata2', 'wild-type', 'IDH', 'TET1/2/3', 'RIPK1', 'and/or', 'TET2', 'GATA2', 'miR-17−5p', 'electroporated', 'MCF-7', 'TNF-mediated', 'long-standing', 'CDK5RAP2/Cnn', 'Oct-4-dependent', 'cyclin', 'miR-34', 'rate-limiting', 'SOX2', 'Tumorigenesis', 'apoptotic', 'miR-21', 'oncogenesis]', 'pRB', 'checkpoints', 'WT1', 'BRaf', 'scrambled-miRVec', 'RHIMs)', 'chromatin-remodeling', 'co-transfected', 'Wts2', 'PPP', 'tumorigenesis', 'Arp', 'microarray', 'TET', 'NSCLCs', 'SOX-17', 'GEFs', 'K-Ras', 'deregulation', 'Thr288'}\n",
      "293\n"
     ]
    }
   ],
   "source": [
    "print(unknown_words)\n",
    "print(len(unknown_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, SimilarityFunction\n",
    "from sentence_transformers.losses import CoSENTLoss\n",
    "\n",
    "# Load a model to train/finetune\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=\"cuda\")\n",
    "\n",
    "# Initialize the CoSENTLoss\n",
    "# This loss requires pairs of text and a float similarity score as a label\n",
    "loss = CoSENTLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_, test_set_, name=\"BIOSSES_test\"):\n",
    "    test_evaluator = EmbeddingSimilarityEvaluator(\n",
    "        sentences1=test_set_[\"sentence1\"],\n",
    "        sentences2=test_set_[\"sentence2\"],\n",
    "        scores=test_set_[\"score\"],\n",
    "        name=name,\n",
    "    )\n",
    "    return test_evaluator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s1 = cleaned_train['sentence1']\n",
    "train_s2 = cleaned_train['sentence2']\n",
    "validation_s1 = cleaned_validation['sentence1']\n",
    "validation_s2 = cleaned_validation['sentence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_emb1 = model.encode(train_s1, normalize_embeddings=True)\n",
    "# train_emb2 = model.encode(train_s2, normalize_embeddings=True)\n",
    "# validation_emb1 = model.encode(validation_s1, normalize_embeddings=True)\n",
    "# validation_emb2 = model.encode(validation_s2, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_similarities = model.similarity_pairwise(validation_emb1, validation_emb2)\n",
    "# print(validation_similarities)\n",
    "ground_truth_validation = cleaned_validation[\"score\"]\n",
    "# print(ground_truth_validation)\n",
    "# print(loss.forward(\n",
    "#     {\"sentence1\": cleaned_validation[\"sentence1\"],\n",
    "#      \"sentence2\": cleaned_validation[\"sentence2\"]}, \n",
    "#     ground_truth_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6227, 0.7996, 0.4542, 0.7373, 0.5717, 0.9421, 0.4272, 0.3671, 0.2034,\n",
      "        0.7460, 0.5736, 0.5206, 0.8329, 0.6441, 0.5521, 0.7422, 0.3377, 0.7593,\n",
      "        0.4291, 0.3967])\n",
      "[0.20000004768371582, 0.7999999523162842, -0.30000001192092896, 0.6000000238418579, 0.5, 1.0, 0.0, -0.30000001192092896, -0.10000002384185791, 0.7000000476837158, 0.0, 0.0, 0.8999999761581421, 0.7000000476837158, 0.7999999523162842, 0.5, 0.20000004768371582, 0.6000000238418579, 0.20000004768371582, -0.30000001192092896]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "test_similarities = model.similarity_pairwise(\n",
    "    model.encode(cleaned_test['sentence1'], normalize_embeddings=True),\n",
    "      model.encode(cleaned_test['sentence2'], normalize_embeddings=True))\n",
    "print(test_similarities)\n",
    "ground_truth_test = cleaned_test[\"score\"]\n",
    "print(ground_truth_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs=1000\n",
    "per_device_train_batch_size=2\n",
    "learning_rate=1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=\"models/all-MiniLM-L6-v2/\"+str(datetime.datetime.now()).split()[1].split('.')[0].replace(':', '-'),\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=100,\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=False,  # Set to True if you have a GPU that supports BF16\n",
    "    # batch_sampler=BatchSamplers.NO_DUPLICATES,  # losses that use \"in-batch negatives\" benefit from no duplicates\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.05,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=0.05,\n",
    "    # save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=0.05,\n",
    "    run_name='{num_train_epochs}_{per_device_train_batch_size}_{learning_rate}',  # Will be used in W&B if `wandb` is installed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=cleaned_validation[\"sentence1\"],\n",
    "    sentences2=cleaned_validation[\"sentence2\"],\n",
    "    scores=cleaned_validation[\"score\"],\n",
    "    name=\"BIOSSES_validate\",\n",
    ")\n",
    "test_evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=cleaned_test[\"sentence1\"],\n",
    "    sentences2=cleaned_test[\"sentence2\"],\n",
    "    scores=cleaned_test[\"score\"],\n",
    "    name=\"BIOSSES_test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8255906849948111\n",
      "0.8178701842922809\n",
      "0.8034244246101594\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(model, cleaned_test)['BIOSSES_test_pearson_cosine'])\n",
    "print(evaluate_model(model, cleaned_train)['BIOSSES_test_pearson_cosine'])\n",
    "print(evaluate_model(model, cleaned_validation)['BIOSSES_test_pearson_cosine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_board_callback = TensorBoardCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    train_dataset=cleaned_train,\n",
    "    eval_dataset=cleaned_validation,\n",
    "    loss=loss,\n",
    "    args=args,\n",
    "    evaluator=dev_evaluator,\n",
    "    callbacks=[tensor_board_callback]\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BIOSSES_test_pearson_cosine': 0.8331904985026963,\n",
       " 'BIOSSES_test_spearman_cosine': 0.802586556268141}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results = test_evaluator(model)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/all-MiniLM-L6-v2/final/{num_train_epochs}-{per_device_train_batch_size}-{learning_rate}\".format(\n",
    "    learning_rate=learning_rate, num_train_epochs=num_train_epochs, per_device_train_batch_size=per_device_train_batch_size\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8331904985026963\n",
      "0.9704619857775055\n",
      "0.8472240995034808\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(model, cleaned_test)['BIOSSES_test_pearson_cosine'])\n",
    "print(evaluate_model(model, cleaned_train)['BIOSSES_test_pearson_cosine'])\n",
    "print(evaluate_model(model, cleaned_validation)['BIOSSES_test_pearson_cosine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
